# Иерархическая сегментация

Датасет имеет следующую последовательность вложенных классов

```
├── (0) background
└── body
    ├── upper_body
    |   ├── (1) low_hand
    |   ├── (6) up_hand
    |   ├── (2) torso
    |   └── (4) head
    └── lower_body
        ├── (3) low_leg
        └── (5) up_leg
```
Вручную вбивается словарь названный `classes_hierarchy`, который имеет вид:

```
bg: 0
body:
  lower_body:
    low_leg: 3
    up_leg: 5
  upper_body:
    head: 4
    low_hand: 1
    torso: 2
    up_hand: 6
```
Он при помощи вспомогательных функций переводится в словарь `classes_content`, который показывает, какие лейблы содержит каждый класс на каждой ступени иерархии. Его проще сделаь вручную, но при большом числе классов нужно иметь утилиты для его составления, что реализовано в модуле `hierarchical_utils`

```
'level_1': 
    'body': [1, 6, 2, 4, 3, 5],
 'level_3': 
     'low_hand': [1],
      'up_hand': [6],
      'torso': [2],
      'head': [4],
      'low_leg': [3],
      'up_leg': [5]},
 'level_2': 
     'upper_body': [1, 6, 2, 4], 
     'lower_body': [3, 5]
```

Далее будут испробованы две архитектуры сетей - `SegNet` и `UNet` с 3 вариантами лоссов (`CrossEntropy`, `DiceLoss`,`FocalLoss`). 
Каждый лосс может применяться просто для классификации с 7 классами, а может быть обернут в модуль `HierarchicalLoss`, который разбивает задачу классификации на три задачи классификации и считает лосс на каждом уровне, при этом поддерживая веса классов. Вероятности составных классов считаются как сумма вероятностей его состовляющих. Предполагается, что учет иерархической структуры в лоссе поможет повысить качество иерархических метрик.

Помимо этого имеется два варианта прогноза классов по вероятностям. Первый, `SimpleClassSelector`, выбирает один из семи классов при помощи `argmax`, а затем последующие классы определяются однозначно по заданной иерархии классов. Второй метод, `SmartClassSelector`, считает вероятность класса отдельно на каждом уровне иерархии, а только затем применяет `argmax`.

Также есть возможность выбора билинейного и гауссового сглаживания при ресайзе картинок, и опциональные (синхронные) аугментации картинки и таргета из библиотеки `albumentations`.

# Структура репозитория

+ `data.py` - содержит `SegDataset`
+ `hierarchical_utils.py` - две важные функции `aggregate_probs` и `decompose_mask`, первая суммирует вероятности для получения вероятности составных классов, а вторая раскладывает максу из 7 классов на 3 маски для каждого уровня
+ `loss.py` - содержит класс `HierarchicalLoss`, а также лоссы `DiceLoss`,`FocalLoss` с поддержкой весов классов. Также там лежит утилита подсчета весов классов как обратной частоты.
+ `metric.py` - реализация метрики `Mean Intersection over Union`
+ `nested_dict.py` - набор утилит для превращения словаря `classes_hierarchy` в `classes_content`
+ `plotting.py` - вроде понятно
+ `selector.py` - реализация селекторов класса
+ `train.py`, `validate.py` - функции обучения модели и подсчета метрик (обновление среднего и std в онлайн-режиме)

Модуль `plotting.py` позволяет создавать картинки наподобие этой:

![image](https://github.com/user-attachments/assets/a0a52111-8525-4910-b563-dc75a006be7f)


# Формат экспериментов

В отдельную папку на Google Drive сохраняются чекпоинты моделей с отметкой названия эксперимента и времени его запуска. В этой папке с целью избежания путаницы также хранится конфиг следующего вида:

```
augmentations: null
batch_size: 16
loss: CrossEntropyLoss
lr: 0.001
model_params:
  __class__: UNet
  act_class: ReLU
  block_depth: 3
  bottleneck_channels: 128
  channels_array:
  - 64
  - 128
  - 256
  - 512
  conv_kernel_size: 3
  enable_batchnorm: true
  in_channels: 3
  out_channels: 7
  pool_kernel_size: 2
resample: 2
```
